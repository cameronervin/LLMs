{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Implementing a Simple LangChain"]},{"cell_type":"markdown","metadata":{},"source":["Installing the necessary imports"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-27T16:15:56.514477Z","iopub.status.busy":"2024-09-27T16:15:56.513281Z","iopub.status.idle":"2024-09-27T16:16:29.076232Z","shell.execute_reply":"2024-09-27T16:16:29.074879Z","shell.execute_reply.started":"2024-09-27T16:15:56.514423Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stdout","output_type":"stream","text":["OPENAI_API_KEY ·······························································································\n"]}],"source":["NEW_ENV = True\n","\n","if NEW_ENV:\n","    %pip install -q langchain langchain_community langchain_openai langchain_chroma sentence_transformers\n","\n","import os\n","import getpass\n","import pandas as pd\n","\n","from langchain_openai import ChatOpenAI\n","from langchain_community.document_loaders import TextLoader\n","from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n","from langchain_text_splitters import CharacterTextSplitter\n","from langchain_core.documents.base import Document\n","from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n","from langchain_chroma import Chroma\n","from langchain.prompts import ChatPromptTemplate\n","from operator import itemgetter\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY\")"]},{"cell_type":"markdown","metadata":{},"source":["Setting up our OpenAI connection"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T16:16:29.079197Z","iopub.status.busy":"2024-09-27T16:16:29.078764Z","iopub.status.idle":"2024-09-27T16:16:29.112846Z","shell.execute_reply":"2024-09-27T16:16:29.111739Z","shell.execute_reply.started":"2024-09-27T16:16:29.079152Z"},"trusted":true},"outputs":[],"source":["llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"]},{"cell_type":"markdown","metadata":{},"source":["Import Yelp review data"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T16:16:29.114561Z","iopub.status.busy":"2024-09-27T16:16:29.114138Z","iopub.status.idle":"2024-09-27T16:16:29.153610Z","shell.execute_reply":"2024-09-27T16:16:29.152345Z","shell.execute_reply.started":"2024-09-27T16:16:29.114522Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"/kaggle/input/yelp-reviews/yelp_reviews.csv\", encoding=\"latin-1\")\n","df[\"reviewer_and_review\"] = df[\"Reviewer Name\"] + \" | \" + df[\"Review\"]"]},{"cell_type":"markdown","metadata":{},"source":["Creating the vector database and embedding the documents"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T16:16:29.155265Z","iopub.status.busy":"2024-09-27T16:16:29.154916Z","iopub.status.idle":"2024-09-27T16:16:58.167249Z","shell.execute_reply":"2024-09-27T16:16:58.166024Z","shell.execute_reply.started":"2024-09-27T16:16:29.155231Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/1740846967.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n","  db = Chroma.from_documents(documents, SentenceTransformerEmbeddings(model_name=embedding_model))\n","/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm, trange\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ef3be876afc4f95b8acf2304b261197","version_major":2,"version_minor":0},"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"811107356c2d4daba06f334b6f65be7f","version_major":2,"version_minor":0},"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"939f2fa813914bc3ba1b09d8f9389dc0","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6480cdae65614defb74db25c30c812a5","version_major":2,"version_minor":0},"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ffe175a9b6c49ec8a1f96af3e121040","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89e588eafc584f41a90d145a5c3eeac9","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c49b3f65bc342b28817945919976ffa","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1b3e20f800542a489461c34b09d1161","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e1209adf7d624096a54cefcd0e60982a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"050b1dbfb4de4bd09389e7db41548f78","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c5e24aed1c548b7b20bc68d9fdcdb98","version_major":2,"version_minor":0},"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["documents = [Document(review) for review in df[\"reviewer_and_review\"]]\n","embedding_model = \"all-MiniLM-L6-v2\"\n","db = Chroma.from_documents(documents, SentenceTransformerEmbeddings(model_name=embedding_model))"]},{"cell_type":"markdown","metadata":{},"source":["Creating the retriever"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T16:16:58.172609Z","iopub.status.busy":"2024-09-27T16:16:58.171306Z","iopub.status.idle":"2024-09-27T16:16:58.177941Z","shell.execute_reply":"2024-09-27T16:16:58.176711Z","shell.execute_reply.started":"2024-09-27T16:16:58.172563Z"},"trusted":true},"outputs":[],"source":["retriever = db.as_retriever(\n","    search_type=\"similarity_score_threshold\", # customize the search_type\n","    search_kwargs={\n","        \"score_threshold\": 0.1, # set the threshold: all similarity scores above this threshold will be included\n","        \"k\": 5 # set the max number of elements to retrieved, regardless of the above threshold.\n","    }\n",")\n","\n","question = \"What did customers have to say about the location?\""]},{"cell_type":"markdown","metadata":{},"source":["Writing the system and user prompts and then compiling the two while utilizing a little bit of prompt engineering"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T16:16:58.180881Z","iopub.status.busy":"2024-09-27T16:16:58.180235Z","iopub.status.idle":"2024-09-27T16:16:58.250864Z","shell.execute_reply":"2024-09-27T16:16:58.249678Z","shell.execute_reply.started":"2024-09-27T16:16:58.180825Z"},"trusted":true},"outputs":[],"source":["system = \\\n","\"\"\"\n","You are a helpful AI bot. You answer a user's question about ice cream store reviews.\n","\n","You have the retrieved the following reviews from a datasource:\n","\n","{reviews}\n","\n","Use these reviews to answer the user's question. Analyze the topic and provide a summary followed by five\n","quotations from the most relevant reviews for the topic. Include the user's name from each of the five reviews to make it easier to\n","locate the specific review later.\n","\n","First include the summary analyzing the users question. This section should have the label \"Summary\"\n","\n","Before the quotations delineate the summary and quotations sections by saying \"Supporting Reviews\"\n","\n","For the quotations, state the quotation number, state the reviewer, a summary of the reviewers sentiments, and the quotation using the following as an example for formatting:\n","The quotation should be the most important component of the review that supports the summary of the review.\n","\n","Review 1\n","Reviewer: Sherry S.\n","Summary: Sherry S. expressed frustration with the service at the park road store, stating that the staff ignored customers and did not provide assistance in taking orders.\n","Quotation: \"just stood there for 15 minutes and the 4 people working there didn't make eye contact or help to take order just were carrying on their conversation.\"\n","\n","\"\"\"\n","\n","human = \\\n","\"\"\"\n","{question}\n","\"\"\"\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", system),\n","    (\"human\", human),\n","])"]},{"cell_type":"markdown","metadata":{},"source":["#### A tangent on different types of prompt engineering\n","\n","Prompt engineering is a crucial technique for guiding the responses of large language models (LLMs). By carefully crafting prompts, you can influence the quality and relevance of the model's output. Here are some common types of prompt engineering techniques along with detailed explanations and examples for each:\n","\n","**1. Assigning Roles**\n","\n","Assigning roles involves giving the model a specific role to play, which helps guide its responses. This technique can be particularly useful when you want the model to adopt a certain perspective or expertise.\n","\n","**Example:**\n","- **Prompt:** \"You are an expert data scientist. Explain the concept of overfitting in machine learning.\"\n","- **Expected Response:** \"Overfitting occurs when a machine learning model captures noise in the training data instead of the underlying pattern. This results in high accuracy on training data but poor generalization to new data.\"\n","\n","**2. One-Shot Prompting**\n","\n","One-shot prompting provides the model with a single example to guide its response. This technique is useful when you want the model to perform a task based on a single instance.\n","\n","**Example:**\n","- **Prompt:** \"Translate the following English sentence to French: 'The cat is on the roof.' Example: 'The dog is in the garden.' -> 'Le chien est dans le jardin.'\"\n","- **Expected Response:** \"'The cat is on the roof.' -> 'Le chat est sur le toit.'\"\n","\n","**3. Few-Shot Prompting**\n","\n","Few-shot prompting gives the model a few examples to learn from before generating a response. This technique helps the model understand the pattern or task more effectively than one-shot prompting.\n","\n","**Example:**\n","- **Prompt:** \n","Translate the following English sentences to French:\n","\n","'The dog is in the garden.' -> 'Le chien est dans le jardin.'\n","'The bird is in the sky.' -> 'L'oiseau est dans le ciel.'\n","'The cat is on the roof.' ->\n","\n","- **Expected Response:** \"'Le chat est sur le toit.'\"\n","\n","**4. Chain of Thought (CoT) Prompting**\n","\n","Chain of Thought (CoT) prompting involves guiding the model to show its reasoning process step-by-step. This technique is particularly useful for tasks that require logical reasoning or multi-step problem-solving.\n","\n","**Example:**\n","- **Prompt:** \"Solve the following math problem step-by-step: What is 15% of 200?\"\n","- **Expected Response:** Step 1: Convert the percentage to a decimal: 15% = 0.15 Step 2: Multiply the decimal by the number: 0.15 * 200 = 30 Answer: 30\n","\n","**5. Providing Context**\n","\n","Providing context involves giving the model additional background information to help it generate a more accurate response. This technique ensures that the model has all the necessary information to understand the query fully.\n","\n","**Example:**\n","- **Prompt:** \"In the context of machine learning, explain the term 'regularization.'\"\n","- **Expected Response:** \"Regularization is a technique used to prevent overfitting by adding a penalty to the model's complexity. This can be done through methods like L1 or L2 regularization, which add a penalty based on the absolute or squared values of the model parameters, respectively.\"\n","\n","**6. Multi-Turn Dialogue**\n","\n","Multi-turn dialogue involves creating a conversation where the model needs to remember and build upon previous interactions. This technique is useful for simulating more natural and coherent conversations.\n","\n","**Example:**\n","- **Prompt:** \n","User: What is the capital of France? Model: The capital of France is Paris. User: What is the population of Paris?\n","\n","- **Expected Response:** \"The population of Paris is approximately 2.1 million people.\"\n","\n","**7. Task-Specific Prompting**\n","\n","Task-specific prompting involves tailoring the prompt to a specific task, such as summarization, text classification, or any other specialized task. This technique helps the model focus on the particular requirements of the task.\n","\n","**Example:**\n","- **Prompt:** \"Summarize the following article in one sentence: [Insert article text here]\"\n","- **Expected Response:** \"[A concise summary of the article]\""]},{"cell_type":"markdown","metadata":{},"source":["Creating the chain"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T16:16:58.252736Z","iopub.status.busy":"2024-09-27T16:16:58.252294Z","iopub.status.idle":"2024-09-27T16:17:04.544766Z","shell.execute_reply":"2024-09-27T16:17:04.543680Z","shell.execute_reply.started":"2024-09-27T16:16:58.252696Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Summary:\n","Customers had mixed experiences at the ice cream store location, with complaints ranging from poor service and rude behavior to unexpected closures without adequate communication or compensation.\n","\n","Supporting Reviews:\n","\n","Review 1\n","Reviewer: Sherry S.\n","Summary: Sherry S. expressed frustration with the service at the park road store, stating that the staff ignored customers and did not provide assistance in taking orders.\n","Quotation: \"just stood there for 15 minutes and the 4 people working there didn't make eye contact or help to take order just were carrying on their conversation.\"\n","\n","Review 2\n","Reviewer: Josue L.\n","Summary: Josue L. mentioned a decline in their favoritism towards the ice cream spot due to rude behavior towards his wife, emphasizing the importance of quality and customer service.\n","Quotation: \"Hey Tutu, you don't get to be rude to my wife and continue to get our business.\"\n","\n","Review 3\n","Reviewer: Sarah M.\n","Summary: Sarah M. expressed disappointment in finding the ice cream shop closed during posted business hours, with no apologies or compensation provided for the inconvenience caused to her and her children.\n","Quotation: \"Closed to prep for a special event. No sorrys, no coupons to come back and try it again. Just a hand-written note and no ice cream.\"\n","\n","Review 4\n","Reviewer: Sarah M.\n","Summary: Sarah M. highlighted the lack of notice or consideration for customers regarding the unexpected closure, questioning how the ice cream shop expects to retain customers under such circumstances.\n","Quotation: \"How do you expect to earn customers when you randomly close with no notice?\"\n","\n","Review 5\n","Reviewer: Josue L.\n","Summary: Josue L. emphasized the competition in the area and how the ice cream shop cannot afford to become complacent, specifically in the crucial areas of quality and customer service.\n","Quotation: \"There's just too much competition out here for you to get comfortable, especially in the two most crucial areas: Quality and Customer Service.\"\n"]}],"source":["# function to process retriever output into a string\n","def docs_to_string(docs_list):\n","    string = \"\"\n","    for doc in docs_list:\n","        string += doc.page_content + \"\\n\"\n","    return string\n","\n","# creating the chain \n","\"\"\"\n","itemgetter retrieves the \"question\" as a string that can be used as input for a retriever\n","RunnableLambda  converts a function into an element that can be used in a chain\n","RunnablePassthrough keeps the existing elements (like \"documents\") and appends the element output\n","\"\"\"\n","chain = (\n","        RunnablePassthrough.assign(reviews= itemgetter(\"question\") | retriever | RunnableLambda(docs_to_string)) \n","        | RunnablePassthrough.assign(prompt=prompt)\n","        | RunnablePassthrough.assign(llm_output= itemgetter(\"prompt\") | llm)\n","        )\n","\n","output = chain.invoke({\"question\": question})\n","print(output[\"llm_output\"].content)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4827770,"sourceId":8160274,"sourceType":"datasetVersion"}],"dockerImageVersionId":30775,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
